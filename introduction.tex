\chapter*{Introduction}

\lettrine[lines=4]{\textcolor{dropcap}{H}}{ow} should one allocate finite resources amongst multiple different projects whose progress is unpredictable in order to achieve the greatest success rate? What is the minimal amount of samples needed to identify the distribution of highest mean amongst a set of unknown distributions? How often should a recommender system suggest completely new objects in order to map out your preferences? Suppose the recommender only knows wether or not you viewed the specific item it suggested. What then is the cost of this information feedback system relative to a perfect information setting? How much more difficult is an allocation problem if the environment is markovian instead of stationary? All these problems can be formulated and answered in the machine learning framework of bandit theory. 

\par Bandit theory is one of the fields of mathematics blessed with an eyebrow-raising name. The reader might find it amusing that this theory has nothing what-so-ever to do with banditry.  The seemingly esoteric name comes from a nickname for slot machines, which are sometimes called {\em one-armed bandits}, for their appearance and the efficiency with which they separate gamblers from their money. A {\em (multi-armed) bandit problem} consists formally of any sequential allocation problem where an {\em agent} receives {\em bandit feedback}, a small amount of information that depends on its actions. They are generally referred to as {\em $K$-armed bandits}, where an agent is faced with a row of $K$ proverbial slot-machines, some of which will have positive pay-off, and where its role becomes to effectively identify and profit from the arms with the highest pay-off. How it carries out this task, its {\em policy}, is the main object of bandit theory. One seeks to find an optimal policy for the specific problem considered

\par There are a very wide range of problems in bandit theory, which fall into three categories. From the most general to the most specific, we first have {\em adversarial bandits}, where no assumptions are made about the slot machines, the reward is set at each round by an opponent which uses a possibly randomised procedure. Second, in {\em Markovian bandits}, each slot machine is a unknown Markov machine, whose state may change at each round. The final case is the special case of {\em stochastic bandits}, where each Markov machine is stationary. As is generally the case in mathematics, the specific case is a simpler problem than the more general formulation. Due to space limitations, this thesis will only provide an introduction to stochastic bandits, as the author feels that any survey of Markovian or adversarial bandits must be supported by a solid foundation in stochastic bandits. 

\par In parallel, there are many settings one can consider in at least one of these three categories which give bandit theory a lot of flexibility to specialise to specific real-world problems. For example, in many real-world applications of information systems, the agent's decisions can be grounded in some external data, called a {\em context}. The family of bandit problems with contexts are thus called {\em contextual bandits}, and have been the subject of a lot of research\cite{banditalgs:11}. There are many other examples, which the reader may find of interest, such as: bandits with side observation\cite{mannor:2011}, bandits on combinatorial structures\cite{kveton:2014}, duelling bandits\cite{yue:2009}, bandits with delayed feedback\cite{dudik:2011}. We will not cover any specialised setting and simply present the regular stochastic bandit for the same reasons as our restriction to the stochastic case. 

\par Having restricted ourselves to the simple stochastic bandit, let us now explicitly outline what this framework is. An {\em instance} of the stochastic $K$-armed bandit problem consists of an agent, which follows a {\em policy}, which takes each turn $t\in\NN$ exactly one action in a set $\mathcal{K}$ of size $K$, immediately receives a reward $X_t$ for the arm it played and then moves to round $t+1$. We will generally further assume that arms are independent and rewards from the same arm are also independent, that reward information is not altered, and that all arms are playable each round. We will also assume (unless otherwise specified) that rewards are bounded\footnote{This issue will be nuanced in section \ref{subsec:mean}, by a class of low variance random variables. However, in general we will state results for bandits with rewards bounded in $[0,1]$. Note that any bounded reward can be rescaled to $[0,1]$.}.

\par We group together all such instances by their number of arms $K$ into the classes $\mathcal{E}_K$, and let $\mathcal{E}$ be the class of all these instances. There are a few implicit assumptions which relate more specifically to the situations where bandits can be used. For instance, note that for the reward distributions to remain the same the overall goal for which the agent was developed must not change either. These are mostly implementation issues, however, and we won't pay them much heed here. 

%history section

\par Before we begin the main discussion of this thesis, the author would like to invite the reader to pause with him and explore the history of the field of bandit theory. While inherently interesting, at least to the author, the reason to do so is to understand how bandit theory fits into the wide picture of machine learning, sampling and experimental design, project management, and control theory, to cite only a few. This review of the literature intends to highlight connections to other fields, evolutions of methods, and provide some answers to the questions which opened this text. 

\par The first work in bandit theory is considered to be \citet{thompson:1933}, which involved the design of sequential clinical trials for two drugs with the goal of designing strategies to minimise the number of people given the least effective drug. Taking as reward the survival or death of each patient this is a stochastic $2$-armed bandit with Bernoulli arms. While Thompson's work did not lead to an immediate flurry of works in bandit theory, his Bayesian approach proved remarkably effective and is still studied today\cite{agrawal:2012}. The IID stochastic bandit, which is our subject in this thesis, was formally introduced in \citet{robbins:1952} building on the sequential design work of Wald. The most important result in stochastic bandits came three decades later in \citet{lai-robbins:1985}, which proved asymptotical guarantees for the performance of any policy an agent could use. We will discuss these, as well as the optimal policy they propose in this thesis. This proof was then extended by \citet{burnetas:1996} to a larger family of distributions, while the policy was named UCB and improved by \citet{auer:2002}. 

\par In parallel, during the 1970s and 1980s the Markovian bandit was extensively studied as a framework to manage allocation of funds to several different research projects\cite{gittins:1989}, led by the work of \citeauthor{gittins:1979}. His seminal policy, the Gittins index\cite{gittins:1979},  gives an optimal Bayesian strategy for the Markovian bandit. what is interesting to note in this work is that it uses discounted future rewards instead of immediate reward, in the context of a Markovian environment. As the reader will see this is not the approach taken in stochastic bandits, or more subtly, it is a generalisation of that approach. Formally, the discounted reward at time $t$ is defined as $\sum_{s\geq t}\gamma^sX_s$, with a positive discount factor $\gamma$. Setting $\gamma=0$, with $0^0=1$ by convention, we recover the immediate reward. This generalisation, along with the previous remark that stochastic bandits are stationary Markovian bandits, should raise the reader's suspicion. These facts are symptoms of the fact that multi-armed bandits are sub-family of problems belonging to the larger family of {\em Markov Decision Processes (MDP)}. These are fundamentally {\em control theory} problems, where an agent has to interact with a Markovian environment which provides some feedback for actions. Bandits are specifically {\em Partially Observable MDP (POMDP)}, as the feedback received is only part of the information about the system. 

\par Is bandit theory, then, a sub-field of control theory? The picture isn't so clean cut. MDPs originated in control theory, but their nature as learning problems with inherent reward system attracted the interest of the machine learning community early on. MDP are a natural general formulation\cite{sutton:1998} for {\em reinforcement learning}, one of the main three paradigms of machine learning, and, while not their explicit original purpose, which was to control dynamic stochastic systems, they have been somewhat absorbed into machine learning. Characterising bandits as a reinforcement learning problem is meaningful as they have all the required components, and are also widely applied to problems relying on information technologies such as ad-serving\cite{vernade:2017,dudik:2011}.  However, it is interesting to remember that bandit theory is older than the first explicit work in machine learning\cite{rosenblatt:1958,samuel:1959}, and derives from a very simple and universal problem of sequential allocation. Doubtless, the omnipresence of the sequential decision problem is one of the reasons bandits have been so successful, along with their flexibility, a the final key component of which we have not yet addressed.

\par The youngest member of the bandit family, and not to be ignored, is the strange case of the adversarial bandit. Introduced by \citet{auer:1995}, the key to this framework is to drastically eliminate all assumptions of Markovian or stochastic bandits, recognising that real world applications tend to grossly violate assumptions such as independence. This might sound like it would make bandit problems virtually infeasible, but this is not the case. In fact, these problems are not much more difficult than the worst case stochastic bandit problem one could imagine\cite{bubeck:2012}. Taking the simple assumption that the adversary in this context has a policy of his own, which doesn't use knowledge of our policy, one can achieve good theoretical results. More importantly, this framework has allowed much better real world results and contributed to the success of bandit theory. The range of applications that are be covered by contemporary bandit theory is a testament to this success. Beyond ad-serving, multi-armed bandits have shown effectiveness for instance in recommender systems, sometimes using similarity graphs between items to improve performance\cite{valko:2014}, packet routing\cite{awerbuch:2004}, search engines\cite{yue:2009}, influence maximisation\cite{vaswani:2015}.  


\par In order to provide an exhaustive presentation of the regular stochastic bandit problem, we will begin by a quick outline of foundational material in measure theory which will allow us to redefine probability, random variables, integration and finally density. We will repeat this exercise with information theory, presenting a quantity of the difference between two distributions, and some inequalities related to it. With these foundational principles we will then cover general notions of bandit theory, our measure of performance and concentration inequalities, followed by general results for performance in the regular stochastic bandit problem. These general performance results will allow us to benchmark two classes of algorithms in the next section, with an analysis of their respective regrets.  Should the reader require them at any moment, a notational appendix and the bibliography are to be found at the end of this document. 



