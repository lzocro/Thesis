\chapter*{Introduction}
\lettrine[lines=4]{\textcolor{dropcap}{H}}{ow} often should a recommender system suggest completely new objects in order to map out your preferences? Suppose the recommender only knows whether or not you viewed the specific item it suggested. What then is the cost of this information feedback system relative to a perfect information setting? What is the minimum amount of samples needed to identify the distribution of highest mean amongst a set of unknown distributions? How can one measure the effectiveness of a targeted advertising algorithm? How should one allocate finite resources amongst different projects in order to maximise profits?  More interestingly, why are these problems be grouped together here? Consider the recommender system: it must suggest objects you like as often as possible, to ensure your continued use of the service, while still suggesting new things to attempt to discover new preferences. This is a fundamental trade-off in many problems called the {\em exploration-exploitation dilemma} which underpins the problems of a field of mathematics known as {\em bandit theory}. All the questions above can in fact be formulated within this theory, in spite of their apparent differences. 

\par Bandit theory is one of the fields of mathematics blessed with an eyebrow-raising name. The reader might find it amusing that this theory has nothing whatsoever to do with banditry.  The seemingly esoteric name comes from a nickname for slot machines, which are sometimes called {\em one-armed bandits}, for their appearance and the efficiency with which they separate gamblers from their money. A {\em (multi-armed) bandit problem} consists formally of any sequential allocation problem where an {\em agent} receives {\em rewards} $X_t$ and {\em bandit feedback}, a small amount of information, based on its actions. They are generally referred to as {\em $K$-armed bandits}, where an agent is faced with a row of $K$ proverbial slot-machines, some of which will have positive pay-off, and where its role becomes to effectively identify and profit from the arms with the highest pay-off. How it carries out this task, its {\em policy}, is the main object of bandit theory. One seeks to find an optimal policy or class of policies for the specific problem considered, and quantify what this optimality means in practice.

\par There are many different types of bandits within bandit theory. The type of a bandit can be decomposed into two qualitatively different attributes: a paradigm and a specific setting. These are different in that paradigms are mutually exclusive, whereas one can consider the same setting in several paradigms. We will begin by outlining the three main paradigms, by the way of three real-world examples which motivate their use. 

\par First, consider the project management problem outlined previously. To make the problem more interesting, each month, the head of the department will allocate funding to one project, but each time a project is funded, the state of the project will change, perhaps making it less worthwhile to fund next month. This would be a very simple formulation of a {\em Markovian bandit}. Here, each project is a proverbial slot-machine {\em arm}, and is represented by a hidden Markov machine. The original problem for which this was designed was in fact the allocation of funds to research projects\cite{gittins:1989}. The optimal policy for this setting, the Gittins index, was first given by \citet{gittins:1979}. Markovian bandits allow us to connect bandit theory to its parent segment of machine learning, {\em reinforcement learning}. Inspection of Gittins' work will bring to the reader's attention that he is working on maximising at each time step {\em expected discounted rewards}, formally defined as, at time $t$, $\sum_{s=0}^n\gamma^sX_{t+s}$, with a positive discount factor $\gamma$. This maximisation problem takes place in an environment called a {\em Partially Observable Markov Decision Process (POMDP)}. Initially, POMDP were mainly a problem in stochastic control theory, but were largely co-opted into reinforcement learning as they are a natural formulation\cite{sutton:1998} for it. 

\par Secondly, consider the sequential clinical trial design problem. Given two new drugs, the traditional way of testing them is to assign to each a fixed number of test subjects and then pick the most effective drug after the trial ends. The key consideration in this problem is how to sequentially allocate new patients to a drug by using the information of the outcome of past patients to minimise allocations to the least effective one, thereby minimising the number of patients who die in the trial. It would be a rational assumption to consider each patient independent of the other and that the environment is stationary. This is considered the first problem in bandit theory, addressed by \citet{thompson:1933}, who provided a Bayesian sampling strategy still studied today for its effectiveness\cite{agrawal:2012}. Note that the stationarity of the environment is what makes this {\em stochastic bandit} a remarkable special case of the Markovian Bandit. This IID stochastic bandit, which will be our subject in this thesis, was formally introduced in \citet{robbins:1952}. The most important result in stochastic bandits came three decades later in \citet{lai-robbins:1985}, which proved asymptotical bounds for the performance of any policy an agent could use. This result, and their proposed optimal policy will be covered later in this thesis. As we have seen, characterising bandits as a reinforcement learning problem is meaningful, however it is interesting to remember that bandit theory is noticeably older than the first explicit work in machine learning\cite{rosenblatt:1958,samuel:1959}, and derives from a very simple and universal problem of sequential allocation. Doubtless, the omnipresence of the sequential decision problem is one of the reasons bandits have been so successful, along with their flexibility. 

\par Finally, consider the problem of ad-serving, or how an algorithm should allocate to each viewer of a web-page one ad from its collection in order to maximise the likelihood of the viewer clicking on it. This is a classic problem in bandit theory\cite{vernade:2017,dudik:2011}, but is not adequately addressed by either stochastic or Markovian bandits. In real world situations, assumptions such as independence are often violated, and interacting with human behaviour is particularly sensitive to this problem. \citet{auer:1995} introduced the {\em adversarial bandit} to remove nearly all the assumptions of Markovian bandits. Here, at each time step, an adversary chooses the rewards for each arm of the bandit completely removing any assumptions about independence or stationarity in rewards. While this could appear to make bandit problems virtually unsolvable, that is not the case. In fact, these problems are not much more difficult than the worst-case stochastic bandit problem one could imagine\cite{bubeck:2012}. Taking the simple assumption that the adversary in this context has a policy of his own, which doesn't use knowledge of our policy, one can achieve good theoretical results. More importantly, this framework has allowed much better real world results and contributed to the success of bandit theory.

\par Having outlined all three paradigms, the reader might now expect us to detail all the main researched settings. This is unfortunately not feasible as, in a testament to the flexibility of bandit theory, there are simply too many settings.  We will give a very short list of some settings, without detailing their exact framework, as examples, and provide further reading. Each of these settings tends to answer one or more simple real world problems adressed by bandit theory, and improve the achieved performance on these problems. In many problems, for instance, there is some available data that can help the agent improve its decision. This is addressed\cite{banditalgs:11} by {\em contextual bandits}, which can be used in any paradigm, and can incorporate further restrictions such as linearity or sparsity of the context. Similarly, in many problems the feedback is slightly different from canonical bandit feedback. It may be delayed in time, giving {\em delayed-feedback bandits}\cite{dudik:2011}, may include some information about other arms than the one played in {\em bandits with side-observations}\cite{mannor:2011}, or be composed of only comparisons between arms in {\em duelling bandits}\cite{yue:2009}. 

\par Another testament to the success of bandit theory is the range of applications covered by contemporary bandit theory. Beyond ad-serving, clinical trials, and project management, multi-armed bandits have shown effectiveness for instance in recommender systems, sometimes using similarity graphs between items to improve performance\cite{valko:2014}, packet routing\cite{awerbuch:2004}, search engines\cite{yue:2009}, and influence maximisation\cite{vaswani:2015}.  This is far from covering the range of settings and applications in the literature but should convince the reader of the value of bandit theory is far greater than the simple introduction that will be given in this thesis. 

\par This thesis will not be a comprehensive overview of bandit theory; in such a small space the author feels one could not provide an honest and thorough summary of bandit theory that remains accessible. The premise of this thesis shall therefore be to provide the background in both methods and understanding to allow the reader to grasp the key tenets of bandit theory. The author feels that any presentation of Markovian or adversarial bandits would be dramatically lacking without thorough understanding of the simpler stochastic bandit. Therefore, one can only begin an introduction to bandit theory with the stochastic bandit. This reasoning also applies to the various modified settings like contextual bandits. The fundamental difference between two settings is the cost of the difference in information. How then can one understand then significance of the context without the context-less setting to compare it to? We must therefore abandon all complex bandits, and analyse only the regular (or simple) stochastic bandit. However, the reader will note that focusing only on this topic will allow us to delve much deeper into it and ensure that it is built on solid foundations and thoroughly understood.

\par Having restricted ourselves to the simple stochastic bandit, let us now explicitly outline what this framework is. An {\em instance} of the stochastic $K$-armed bandit problem consists of an agent, which follows a {\em policy}, which takes each turn $t\in\NN$ exactly one action in a set $\mathcal{K}$ of size $K$, immediately receives a reward $X_t$ for the arm it played and then moves to round $t+1$. We will generally further assume that arms are independent and rewards from the same arm are also independent, that reward information is not altered, and that all arms are playable each round. We will also assume (unless otherwise specified) that rewards are bounded\footnote{This issue will be nuanced in section \ref{subsec:mean}, by a class of low variance random variables. However, in general we will state results for bandits with rewards bounded in $[0,1]$. Note that any bounded reward can be rescaled to $[0,1]$.}. We group together all such instances by their number of arms $K$ into the classes $\mathcal{E}_K$, and let $\mathcal{E}$ be the class of all these instances regardless of their number of arms. There are a few implicit assumptions which relate more specifically to the situations where bandits can be used. For instance, note that for the reward distributions to remain the same the overall goal for which the agent was developed must not change either. These are mostly implementation issues, however, and we won't pay them much heed here. 

\par In order to provide an exhaustive presentation of the regular stochastic bandit problem, we will begin by a quick outline of foundational material in measure theory which will allow us to redefine probability, random variables, integration and finally density. We will repeat this exercise with information theory, presenting a quantity of the difference between two distributions, and some inequalities related to it. With these foundational principles we will then cover general notions of bandit theory, our measure of performance and concentration inequalities, followed by general results for performance in the regular stochastic bandit problem. These general performance results will allow us to benchmark two classes of algorithms in the next section, with an analysis of their respective regrets.  Should the reader require them at any moment, a notational appendix and the bibliography are to be found at the end of this thesis. 