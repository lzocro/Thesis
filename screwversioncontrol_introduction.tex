\chapter*{Introduction}
\lettrine[lines=4]{\textcolor{dropcap}{\textbf{H}}}{ow} often should a recommender system suggest completely new objects in order to map out your preferences? Suppose the recommender only knows whether or not you viewed the specific item it suggested. What then is the cost of this information feedback system relative to a perfect information setting? What is the minimum amount of samples needed to identify the distribution of highest mean amongst a set of unknown distributions? How can one measure the effectiveness of a targeted advertising algorithm? How should one allocate finite resources amongst different projects in order to maximise profits?  Why and how are these problems related? Consider the recommender system: it must suggest objects you like as often as possible, to ensure your continued use of the service, while still suggesting new things to attempt to discover new preferences. This is a fundamental trade-off in many problems called the {\em exploration-exploitation dilemma} which underpins the problems of a field of mathematics known as {\em bandit theory}. All the questions above can in fact be formulated within this theory, in spite of their apparent differences. 

\par Bandit theory is one of the fields of mathematics blessed with an eyebrow-raising name. The reader might find it amusing that this theory has nothing whatsoever to do with banditry.  The seemingly esoteric name comes from a nickname for slot machines, which are sometimes called {\em one-armed bandits}, for their appearance and the efficiency with which they separate gamblers from their money. A {\em (multi-armed) bandit problem} consists formally of any sequential allocation problem where an {\em agent} interacts with an {\em environment}, receiving {\em rewards} $X_t$ (a {\em bandit feedback}) based on its actions. They are generally referred to as {\em $K$-armed bandits}, where an agent is faced with a row of $K$ proverbial slot-machines, some of which will have positive pay-off, and where its role becomes to effectively identify and profit from the arms with the highest pay-off. How it carries out this task, its {\em policy}, is the main object of bandit theory. One seeks to find an optimal policy or class of policies for the specific problem considered, and quantify what this optimality means in practice. There are many different types of bandits within bandit theory. The type of a bandit can be decomposed into two qualitatively different attributes: a paradigm and a specific setting. Paradigms are assumptions about the nature of rewards, whereas settings are assumptions about available information. We will begin by outlining the main paradigms. 

\par A first approach would be to take usual assumptions of stochastic behaviour of the rewards. For example, consider the sequential clinical trial design problem. Given two new drugs, the traditional way of testing them is to assign to each a fixed number of test subjects and then pick the most effective drug after the trial ends. However applying this method will lead to some unfortunate subjects dying from the lesser drug. In a sequential experiment, in contrast, patients are added in turn to either drug group and their outcome observed before assigning the next patient. The problem of best exploiting this new information framework is considered the first problem in bandit theory. It was proposed by \citet{thompson:1933}, who provided a Bayesian sampling strategy still studied today for its effectiveness\cite{agrawal:2012}. \citeauthor{thompson:1933} assumed that patients' outcomes were independent and that the environment was stationary, a particular set of assumptions called the IID {\em stochastic bandit}\cite{robbins:1952}, but the fundamental feature of a stochastic bandit is simply that the environment generates stochastic rewards. 

\par There is a special case of the stochastic bandit which stands out and is important to mention: the {\em Markovian bandit}. Above, it was assumed that the environment was stationary, i.e. that the reward distributions for each arm were the same throughout. In the Markovian bandit, one allows the environment to operate as a stochastic process, with states that may influence the reward distributions. The original problem for which this was designed was in fact the allocation of funds to research projects\cite{gittins:1989} at regular time intervals. With this in mind, see that the multiple projects (arms) depend on some unknown presumably stochastic process which determines how they progress. This environment is formally called a {\em Partially Observable Markov Decision Process (POMDP)} which is a general formulation\cite{sutton:1998} for {\em reinforcement learning}. While bandits predate the first explicit works in machine learning\cite{rosenblatt:1958,samuel:1959}, they are a textbook example of reinforcement learning as they learn to make decisions via the rewards they obtain from an unknown environment.

\par Consider the problem of ad-serving, or how an algorithm should allocate to each viewer of a web-page one ad from its collection in order to maximise the likelihood of the viewer clicking on it. This is a classic problem in bandit theory\cite{vernade:2017,dudik:2011}, but is not adequately addressed by either stochastic or Markovian bandits. In real world situations, assumptions such as independence are often violated, and interacting with human behaviour is particularly sensitive to this problem. In response, \citet{auer:1995} introduced the {\em adversarial bandit} to remove nearly all the assumptions of stochastic bandits. Here, at each time step, an adversary chooses the rewards for each arm of the bandit completely removing any assumptions about independence or stationarity in rewards. While this could appear to make bandit problems virtually unsolvable, that is not the case. In fact, these problems are not much more difficult than the worst-case stochastic bandit problem one could imagine\cite{bubeck:2012}. Taking the simple assumption that the adversary in this context has a policy of his own, which doesn't use knowledge of our policy, one can achieve good theoretical results. More importantly, this framework has allowed much better real world results and contributed to the success of bandit theory.

\par Having outlined all three paradigms, the reader might now expect us to detail all the main researched settings. This is unfortunately not feasible as, in a testament to the flexibility of bandit theory, there are simply too many settings.  We will give a very short list of some settings, without detailing their exact framework, as examples, and provide further reading. Each of these settings tends to answer one or more simple real world problems addressed by bandit theory, and improve the achieved performance on these problems. In many problems, for instance, there is some available data that can help the agent improve its decision. This is addressed\cite{banditalgs:11} by {\em contextual bandits}, which can be used in any paradigm, and can incorporate further restrictions such as linearity or sparsity of the context. Similarly, in many problems the feedback is slightly different from canonical bandit feedback. It may be delayed in time, giving {\em delayed-feedback bandits}\cite{dudik:2011}, may include some information about other arms than the one played in {\em bandits with side-observations}\cite{mannor:2011}, or be composed of only comparisons between arms in {\em duelling bandits}\cite{yue:2009}. 

\par Another testament to the success of bandit theory is the range of applications covered by contemporary bandit theory. Beyond ad-serving, clinical trials, and project management, multi-armed bandits have shown effectiveness for instance in recommender systems, sometimes using similarity graphs between items to improve performance\cite{valko:2014}, packet routing\cite{awerbuch:2004}, search engines\cite{yue:2009}, and influence maximisation\cite{vaswani:2015}. This is far from covering the range of settings and applications in the literature but should convince the reader that bandit theory reaches far further than the contents of this thesis.

\par The premise of this thesis shall thus be to outline an approachable informative introduction to bandit theory that would allow a reader with a graduate level in mathematics to build solid basics in bandit theory. Unfortunately, in such a small space the author does not feel that all paradigms nor all settings can be addressed with the level of rigour he considers necessary to sustain the premise of this thesis. The question is then: where to begin? The author believes the answer is to begin with the IID stochastic bandit as introduced by \cite{robbins:1952} and presented above. There are several reasons for this: it is a simple framework, it is also the first bandit framework to have been presented, but most importantly it is a foundation which introduces the required components to understand other bandit problems. A good understanding of the simple stochastic bandit enlightens an exploration of adversarial or Markovian bandits as one can understand {\em how} the changes in assumptions lead to changes in the guarantees. The author won't get ahead of himself and give examples here, but the author will give passing remarks once the reader has been made familiar with the characteristics of the stochastic bandit problem.  

\par Having restricted ourselves to the simple stochastic bandit, let us now explicitly outline what this framework is. An {\em instance} of the stochastic $K$-armed bandit problem consists of an agent, which follows a policy, which takes each turn exactly one action in a set of size $K$, immediately receives a reward for the arm it played and then moves to the next round. We will generally further assume that arms are independent and rewards from the same arm are also independent, that reward information is not altered, and that all arms are playable each round. We will also assume (unless otherwise specified) that rewards are bounded\footnote{This issue will be nuanced in section \ref{subsec:mean}, by a class of low variance random variables. However, in general we will state results for bandits with rewards bounded in $[0,1]$. Note that any bounded reward can be rescaled to $[0,1]$.}. There are a few implicit assumptions which relate more specifically to the situations where bandits can be used. For instance, note that for the reward distributions to remain the same the overall goal for which the agent was developed must not change either. These are mostly implementation issues, however, and we won't pay them much heed here. 

\par This thesis aims to contribute an approachable introduction to stochastic bandits which can be understood with a graduate background in probability. Thus to cover required material from other fields, we will begin Chapter \ref{chap:FP} with a quick outline of foundational material in measure theory which will allow us to redefine probability, random variables, integration and finally density. Then we will do the same for information theory, presenting a quantity of the difference between two distributions, and some inequalities related to it. With these foundational principles we will then in Chapter \ref{chap:2} cover general notions of bandit theory, our measure of performance and concentration inequalities, followed by general results for performance in the regular stochastic bandit problem. These general performance results will allow us to benchmark two classes of algorithms in Chapter \ref{chap:algs}, with an analysis of their respective regrets.  Should the reader require them at any moment, a notational appendix and the bibliography are to be found at the end of this thesis. 
















