
%\lettrine[lines=4,slope=1pt]{\textcolor{red}{A}}{ll} 
 %\lettrine[lines=3,slope=1pt]{A}{n} 
%
%
%
\chapter{Foundational Preliminaries}\label{chap:FP}
%
%
%


\lettrine[lines=4,slope=2pt]{\textcolor{dropcap}{\textbf{R}}}{igourous} definitions are an essential prerequisite to any mathematical exercise, and bandit theory is no exception. While not constitutive of the main body of bandit theory, notions outlined in this chapter are required to proceed in an orderly manner in a survey of multi-armed bandits and of the stochastic bandit specifically. This chapter assumes little familiarity with the topics of measure or information theory, neither does it seek to be an introduction to them. It will present only the required components upon which to base the rest of this thesis. We will begin with an exploration of measure theory, defining or redefining the concepts of probability, including integration, density, and measures. We will then immediately apply these building blocks to a succinct brief in Information theory, whose purpose is to present to the reader the Kullback-Leibler divergence and some of its properties.  



%
%
\section{Elements in Measure Theory}\label{sec:MT}
%
%

\lettrine[lines=3,slope=1pt]{\textbf{M}}{easure theory} allows us to reformulate and generalise many statements about probability distributions. This new field refines the theory of probability and allows us to derive a new understanding of what probabilities, events, outcomes, and distributions are. We will assume that the basic notions of sample space and random variables are known to the reader. To go beyond naive probability, we will need to rebuild our definitions from new classes of sets. We will begin by introducing measures, then we will extend them to a key theorem, which will allow us to formulate a generalised probability density, unifying probability mass and density functions, and allowing us to derive new properties of distributions with information theory. Proofs of results will be omitted, but can be found in \citet{leadbetter:2014}.

%
\subsection{Measures}\label{subsec:Measures}
%

\par We consider first an arbitrary topological space $\mathcal{X}$, and are interested in $\mathcal{E}\subset\mathcal{P}(\mathcal{X})$, a class of sets on $\mathcal{X}$. We will take the normal operations on sets, and we will denote set-theoretic difference as ``$-$'' and define some behaviours $\mathcal{E}$ can have. 

\begin{definition}[Rings and Fields]
A non empty class $\mathcal{E}$ is a {\em ring} if for all $E,F\in \mathcal{E}$: $E\cup F, E-F \in \mathcal{E}$. Further, a {\em field} is a ring closed under the complement in $\mathcal{X}$. A ring closed under countable union is called a {\em $\sigma$-ring}, and naturally it is a $\sigma$-field or $\sigma$-algebra if it is also closed under complements. 
\end{definition}

In probability we will be mostly interested in $\sigma$-algebras, but measure theory includes work on more general cases, like rings\cite[p.~23]{leadbetter:2014}. A noteworthy example of a $\sigma$-algebra is the Borel $\sigma$-algebra $\mathcal{B}$ on the real line. Consider the collection of all open intervals on $\RR$, or more generally of all open sets in $\RR$. The class of Borel sets for the real line is the $\sigma$-ring generated by this collection. This can be shown to also be a $\sigma$-algebra, which we denote by $\mathcal{B}$. The class $\mathcal{B}$ contains for instance all one-point and countable sets, as well as all intervals in $\mathbb{R}$. Now that we have clearly defined and explored classes of sets, we will set out properties of set functions and define measures. 

\begin{definition}[Set functions and their properties]
A map $M$ from $\mathcal{E}$ to some set $S$ is a set function if for every $e\in\mathcal{E}$, $M(e)\in S$. We can say that $M$ is:
\begin{list}{$\circ$}{}
\item Non-negative, if for all $E\in \mathcal{E}\, :\, M(E)\geq 0$.
\item Countably additive, if for $\{E_i\}_i$ disjoint sets in $\mathcal{E}$, we have $M(\cup_{i=1}^\infty E_i)=\sum_{i=1}^\infty M(E_i)$.
\item Finite, if for all $E\in \mathcal{E}\, :\, \vert M(E)\vert <\infty$.
\item $\sigma$-finite, if for all $E\in \mathcal{E}$ there is a sequence of sets $\{E_i\}_i$ in $\mathcal{E}$ with $E\in\cup_{i=1}^\infty E_i$ and $\vert M(E_i)\vert <\infty$ for all $i\in\mathbb{N}$.
\item Real-valued, if for all $E\in \mathcal{E}\, :\, M(E)\in\mathbb{R}$.
\item Simple, if $M$ is Real valued, $\mathcal{E}=\mathcal{X}$, and $M(E)$ takes a finite number of values. 
\item Monotone, if for all $E\subset F\in \mathcal{E} \Leftrightarrow M(E)\leq M(F)$.
\item Subtractive, if for all $E\subset F\in \mathcal{E}$ such that $E-F\in\mathcal{E}$ and $\vert M(E)\vert <\infty$, we have $M(F-E)=M(F)-M(E)$.
\end{list}
\end{definition}

\begin{definition}[Measure]
A measure $\mu$ on $\mathcal{E}$, with $\emptyset\in\mathcal{E}$, is a non-negative, countably additive set-function from $\mathcal{E}$ to $\RR$. If $\mu(\mathcal{X})=1$ then $\mu$ is a {\em probability measure}.
\end{definition}

\par One of the most important measures is the Lebesgue measure on $\mathcal{B}$ defined simply as the difference between the bounds of the interval, the intuitive length: $\mu((a,b))=b-a$. The Lebesgue measure further returns the intuitive length, area, and volume when applied to $n$-dimensional euclidian space.

\begin{definition}[Measurability]
Let $(\mathcal{X},\mathcal{S})$ be a measurable space, i.e. $\mathcal{S}$ is $\sigma$-algebra on $\mathcal{X}$, a set $E$ is measurable simply if it is an element of $\mathcal{S}$.
\end{definition}

\par To begin expanding measurability from sets to functions, we introduce the measure space $(\mathcal{X},\mathcal{S},\mu)$, where $\mathcal{X}$ is a topological space, $\mathcal{S}\in\mathcal{P}(\mathcal{X})$ is a $\sigma$-algebra, and $\mu$ is a measure on $\mathcal{S}$. We will also extend our example of the Borel sets to the extended real line $\mathbb{R}^*=\mathbb{R}\cup\{-\infty,\infty\}$. The extended Borel $\sigma$-algebra $\mathcal{B}^*$ is defined as $\{ B, B\cup\{\infty\},B\cup\{-\infty\}, B\cup\{-\infty,\infty\} : B\in\mathcal{B} \}$.

\begin{definition}[$\mathcal{S}$-measurability]
Let $f:(\mathcal{X},\mathcal{S})\mapsto(\mathcal{Y},\mathcal{T})$ be a set-function between two measurable spaces, f is $\mathcal{S}$-measurable if: 
\[\forall E\in\mathcal{T}:\, f^{-1}(E)\in\mathcal{S}\, . \]
When we work with a real valued set-function $f$ we take $(\mathcal{Y},\mathcal{T}):= (\mathbb{R}^*,\mathcal{B}^*)$ in this definition.\end{definition}

\par Notice that the probability measure is a map from the set of all possible events in $\mathcal{X}$, the $\sigma$-algebra $\mathcal{S}$, to the interval $[0,1]$, where $\mu(A)$ is $1$ if and only if $A=\mathcal{X}$. So a measure acts like the probability distribution of a random variable, in that to each possible combination of outcomes from the sample space $\mathcal{X}$ it associates a probability, and that the measure of the union of all events, akin to the sum of the probabilities of each outcome is equal to $1$ by countable additivity. In this subsection we have built only the most basic components of probability theory. In the next subsection we will construct densities using the Radon-Nikodym theorem and measure theoretic integration.


%
\subsection{Integration and Density}
%

\par Before we can introduce the aforementioned theorem, we have to discuss integrability with respect to a measure. Integration is such a key part of probability theory, that it seems only natural to attempt to extend the Riemann integral beyond the real line and to the new classes of sets we have developed. We begin by the case of integrating simple functions, which we can write as a finite sum of indicator functions, where $E_i$ is the sub-class of $\mathcal{E}$ where $f$ takes value $a_i$. Then we can define the (Lebesgue) integral of these functions as a simple finite sum:
\[ \int f \de\mu= \sum_{i=1}^na_i\mu(E_i)\, . \]
This is an important first step as it can be shown that all non-negative measurable functions are the limit of an increasing sequence $\{f_n\}_n$ of non-negative simple functions. This allows us to formulate a sensible definition for the integral of such functions:
\[\int f \de \mu:=\lim_{n\to \infty} \int f_n\de\mu\, . \]
If the integral as such defined is finite, $f$ is integrable. This definition is consistent with the Riemann integral of positive functions on the real line, but it needs to be extended to functions with values in $(-\infty,0)$. This is done by separating the function into positive and negative parts $f_+$ and $f_-$. See now that both are non-negative and if they are integrable we have that $f$ is integrable and its integral takes the finite value:
\[\int f\de\mu:=\int f_+\de\mu - \int f_-\de\mu\, .\]

\par To proceed to more interesting properties, we will need to define the term ``{\em almost everywhere}'' in regards to a statement $s$. In a fixed measure space, %$s$ is said to hold almost everywhere if it holds for a set $A\in\mathcal{S}$ such that $\mu(A^c)=0$. 
if $s$ holds on all $\mathcal{S}$ except for a set of measure $0$, then $s$ is said to hold almost everywhere.
In terms of the measure, the collection of points where the property does not hold is negligible, which gives the otherwise ill-defined ``almost everywhere'' a sensible meaning. Theorem \ref{thm:prop_int_measures} collects two simple but useful properties of integrable functions.

\begin{theorem}[{ \citethm[p.~69]{leadbetter:2014}}]\label{thm:prop_int_measures}%4.4.1 in leadbetter
If $f$ is integrable with respect to a measure $\mu$, it is finite almost everywhere with respect to $\mu$. Furthermore, if $g$ is measurable, defined almost everywhere and $E$ is a set with $0$ measure, then $\int_E g \de\mu=0$. 
\end{theorem}  

These results are important implicitly in the Radon-Nikodym theorem, but we must clarify a few terminology details before presenting the main result of this sub-section.

\begin{definition}\hfill 
\vspace{-1.3em}
\begin{list}{$\circ$}{}
\item Absolute continuity: $\nu$ is absolutely continuous with respect to $\mu$, denoted $\nu\ll\mu$, if $\mu(E)=0\Rightarrow\nu(E)=0$
\item Essentially unique: If $f$ is essentially unique with respect to a property, then any function $g$ with this property is equal to $f$ almost everywhere.
\end{list}
\end{definition}

\par The main result in this section is outlined in Theorem \ref{thm:RN}, the Radon-Nikodym theorem. In an intuitive sense it defines a density function whose integral over a set is the measure of the set. This can be seen as some analog to integrating a density function to obtain an evaluation of the cumulative density function. This result however is much stronger, owing to the use of two measures in the formula, which will allow us to derive in Theorem \ref{thm:RN-derivative} a method for changing the measure in an integral. 


\begin{theorem}[Radon-Nikodym-Theorem for measures, {\citethm[p.~100]{leadbetter:2014}}]\label{thm:RN}
Let $(\mathcal{X},\mathcal{S},\mu)$ be a $\sigma$-finite measurable space, and let $\nu$ be a $\sigma$-finite measure on $\mathcal{S}$. If $\nu\ll\mu$, then there is an essentially unique finite-valued non-negative measurable function $f$ on $\mathcal{X}$ such that:
\[ \forall E\in\mathcal{S}\,:\, \nu(E)=\int_E f\de\mu\,. \]
\end{theorem}
\begin{theorem}[{\citethm[p.~102]{leadbetter:2014}}]\label{thm:RN-derivative}
Let $\mu,\nu$ be $\sigma$-finite measures of $(\mathcal{X},\mathcal{S})$, with $\nu\ll\mu$. If $f$ is a measurable function defined on $\mathcal{X}$ and is either $\nu$-integrable or non-negative, then:
\[\int f \de\nu =\int f\hspace{1mm}\frac{\de\nu}{\de\mu}\hspace{1mm}\de\mu\,. \]
\end{theorem}

In Theorem \ref{thm:RN-derivative}, $\frac{\de\nu}{\de\mu}$ is called the Radon-Nikodym derivative of $\nu$ with respect to $\mu$. A particular extension of these new derivatives, like in Calculus, is the definition of a ``{\em chain rule}''. Theorem \ref{RN-chain} gives this property, which will allow us to write a density as the product of other densities. 

\begin{theorem}[``Chain rule'' for measures, {\citethm[p.~103]{leadbetter:2014}}]\label{RN-chain}
Let $\mu$, $\nu$, and $\lambda$ be $\sigma$-finite measures on $(\mathcal{X},\mathcal{S})$. Then if $\lambda\ll\nu\ll\mu$, we have almost everywhere with respect to $\mu$:
\[ \frac{\de\lambda}{\de\mu}=\frac{\de\lambda}{\de\nu}\frac{\de\nu}{\de\mu}\, .\]
\end{theorem}


\par This new formulation of a probability density or mass function, concludes our overview of measure theory. We will use the notation, definitions and results throughout this thesis, and in particular we will immediately apply them in an overview of relevant information theory concepts.


%
%
\section{Elements of Information Theory}\label{sec:IT}
%
%
\lettrine[lines=3,slope=0pt]{\textbf{E}}{merging} in the flurry of new research domains to arise in the wake of the Second World War, {\em information theory} studies the transmission of messages, and the amount of information they contain. We will build upon the original purpose of information theory to illustrate its fundamentals. Using the results from section \ref{sec:MT}, we will diverge from the original works of \citeauthor{Shannon:1948}, and consider applications beyond the transmission of messages. We will ignore the notion of entropy, and present the problem from a hypothesis testing angle, outlined by Kullback\cite{Kullback:1997}. This is because we do not truly need core information theory, but only the parts which transfer to probability theory, in particular the {\em Kullback-Leibler divergence}. While it is often defined as relative {\em entropy} in information theory, this connection will not be explored; should the reader like a deeper view in the subject we recommend Cover and Thomas\cite{Cover:2012}, Feinstein\cite{Feinstein:1958}, or Shannon\cite{Shannon:1948} himself.


%
\subsection{Information and Divergence}
%

\par The fundamental premise of information theory is in signal processing and deals with the analysis of the transmission of messages over \textit{noisy} channels\cite{Ash:1965}. To by-pass the random scrambling of characters in the channel, the two parties can agree on a common \textit{code}. The source then encodes its message into an object, such as a vector in some vector space, which is transmitted over the channel to the destination's decoder. In the real world, and in signal processing, there is a cost to a more complicated code in terms of the speed of transfer of the message but in applications to statistics we are interested in the point of view of the decoder not the transmission of a message. Instead of a message, say we receive a random value $x$ and we are tasked with determining if it comes from distribution $f_1$ of $f_2$. What amount of \textit{information} does $x$ contain about differentiating $f_1$, $f_2$?  In this thought experiment, taken and adapted from \citet{Kullback:1997}, let us formulate using Bayes' theorem the posterior probability of a hypothesis $i=1,2$. corresponding to $x$ belonging to $f_i$.:
\[P(H_i\vert x) = \frac{P(H_i)f_i(x)}{P(H_1)f_1(x)+P(H_2)f_2(x)}\, . \]
We can rearrange the logarithm of the ratio of posteriors for $i=1,2$ into a formula that holds almost everywhere with respect to $\mu$:
\[ \ln\frac{f_1(x)}{f_2(x)}=\ln\frac{P(H_1\vert x)}{P(H_2\vert x)}-\ln\frac{P(H_1)}{P(H_2)}\, . \]
\par Since the right-hand side is a measure of the change in log-odds between $H_1$, $H_2$ before and after $x$ is observed, the left hand side is too, albeit in a less obvious way. We call the left-hand side the information contained in $x$ for differentiating $H_1$ from $H_2$.  We can then define the mean information using the generalised probability densities $f_1$, $f_2$ for $E$ such that $\mu_1(E)\neq0$:
\begin{align*}
 I(1,2;E)&=\frac{1}{\mu_1(E)}\int_E \ln\frac{f_1(x)}{f_2(x)}\de\lambda_1\\
 &=\frac{1}{\mu_1(E)}\int_E f_1(x)\ln\frac{f_1(x)}{f_2(x)}\de\lambda_2\, .
\end{align*}
The second line follows from the Radon-Nikodym derivative $f_1(x):=\frac{\de\lambda_1}{\de\lambda_2}(x)$. Here we notice that the $f_i$ are not important and instead can be fully described by three metrics, $\lambda_1$, $\lambda_2$, and $\mu$. Thus in the measurable space $(\Omega,\mathcal{F})$ we apply $\mu$ to make it a probability space, and two other measures $\lambda_i$ such that the Radon-Nikodym derivatives of the $\lambda_i$ with respect to $\mu$ are the densities we are trying to separate. Extending our definition from $E$ to $\Omega$:
\[D_{KL}(\lambda_1\Vert\lambda_2):= \int \ln\frac{\de\lambda_1}{\de\lambda_2}\de\lambda_1=\int \frac{\de\lambda_1}{\de\lambda_2} \ln\frac{\de\lambda_1}{\de\lambda_2}\de\lambda_2\, . \]
This quantity is called the Kullback-Leibler (KL) divergence. Note that if $\lambda_1$ is not absolutely continuous with respect to $\lambda_2$, the divergence is considered infinite, but otherwise it is finite. This operator is not symmetric and it is therefore not possible to think of the divergence between two measures, but rather from $\lambda_2$ to $\lambda_1$. The most sensible definition is doubtless the idea which we developed in the simple case as the significance of how helpful information is at separating one measure from the other.  We do not need further details about the properties of the Kullback-Leibler divergence, or about other information theoretic concepts such as entropy. As the reader might have guessed, we are mostly interested in using the probability measures of specific random variables, rather than arbitrary ones. Throughout this thesis an important exercise will be the bounding of random variables, and for one such bound we will require a class of results known as the \textit{Pinsker-type inequalities}. 

%
\subsection{Pinsker-type Inequalities}
%
 
 \par One reason we have developed all these precise tools, is to form an acceptable background for establishing and proving specific results. Our preeminent concern will be the discovery of bounding inequalities. An important bounding equality related to Kullback-Leibler divergence is the Pinsker inequality. For two measures in the measurable space $(\Omega,\mathcal{F})$ as before, we define the total variation distance $\delta(\lambda_1,\lambda_2)=\sup\{ \vert \lambda_1(E) - \lambda_2(E)\vert : E\in\mathcal{F}\}$. Then the  Pinsker inequality states that:
 \[ 2\delta(\lambda_1,\lambda_2)^2 \leq D_{KL}(\lambda_1\Vert \lambda_2)\, . \] 
This subsection consists in the derivation of a bound for the sum of measures of two complementary events, which we will refer to as the {\em Pinsker-type} inequality as it contains the Kullback-Leibler divergence like the true Pinsker inequality. This identity is given in Theorem \ref{thm:PTI} and is proven immediately afterwards. 

\begin{theorem}[Pinsker-type inequality\cite{banditalgs:6}]\label{thm:PTI}
Let $\lambda_1,\lambda_2$ be probability measures on $(\Omega,\mathcal{F})$. For all $E\in\mathcal{F}$, we have:
\[\lambda_1(E)+\lambda_2(E^c)\geq \frac{1}{2}\exp(-D_{KL}(\lambda_1\Vert \lambda_2))\, . \]
\end{theorem}

\begin{proof}
Consider a probability space $(\Omega,\mathcal{F},\mu)$, let $\lambda_1$ and $\lambda_2$ be two further measures on this space, respectively with Radon-Nikodym derivatives  $f_1$ and $f_2$ with respect to $\mu$. For $E\in\mathcal{F}$, we begin by reducing the left hand side to an integral of a minimum on $\Omega$. See that:
\begin{align*}
\lambda_1(E)+\lambda_2(E^c) &= \int_Ef_1\de\mu + \int_{E^c}f_2\de\mu\\
&\geq \int_E \min(f_1,f_2)\de\mu + \int_{E^c}\min(f_1,f_2)\de\mu\\
&\geq \int \min(f_1,f_2)\de\mu\,. 
\intertext{Note now that $f_1+f_2=\max(f_1,f_2)+\min(f_1,f_2)$. Using the unit integrability of measures this trivial fact allows us to derive the much more useful statement that $\int \max(f_1,f_2)\de\mu\leq 2-\int\min(f_1,f_2)\de\mu\leq 2$. Now:}
\int \min(f_1,f_2)&\geq \frac{1}{2}\int\min(f_1,f_2)\de\mu\int\max(f_1,f_2)\de\mu \\
&\geq \frac{1}{2}\int f_1\de\mu\int f_2\de\mu \\
&\geq \frac{1}{2}\int \left(\sqrt{f_1f_2}\right)^2\de\mu\\
&\geq \frac{1}{2}\left(\int\sqrt{f_1f_2}\de\mu\right)^2\\
&\geq \frac{1}{2}\exp\left( 2\ln\int\sqrt{f_1f_2}\de\mu \right)\\
&\geq \frac{1}{2}\exp\left( 2\ln\int f_1\sqrt{\frac{f_2}{f_1}}\de\mu \right)\,.
\intertext{Further as $f_2$ is absolutely continuous with respect to $f_2$, we know that $f_1>0$ implies $f_1f_2>0$. This will allow us to apply Jensen's inequality:}
\lambda_1(E)+\lambda_2(E^c)&\geq \frac{1}{2}\exp\left( 2\int f_1\ln\sqrt{\frac{f_2}{f_1}}\de\mu \right)\\
&\geq \frac{1}{2}\exp\left( -\int f_1 \ln \frac{f_1}{f_2}\de\mu \right)\,.
\intertext{Replacing $f_1$ and $f_2$ by their definition as Radon-Nikodym derivatives yields the result:}
\lambda_1(E)+\lambda_2(E^c)&\geq \frac{1}{2}\exp\left( -D_{KL}(\lambda_1\Vert \lambda_2)\right)\,.
\end{align*}
\end{proof}

\par Throughout this chapter, we have reformulated probability theory in terms of topological spaces, $\sigma$-algebras and measures. By redefining functions on measurable spaces we developed a formalism which is consistent with the axioms of probability, but provides us with new flexibility. This new framework led us to new insights about the definition of densities, unifying the countable and uncountable domains. Further, we derived a new calculus for measures, using the Radon-Nikodym theorem, and Lebesgue integration, giving rise in the measure theoretic framework to new concepts of continuity and uniqueness. These ideas will be used throughout this thesis, and notably we applied them straight away to a probabilistic exploration of information theoretic concepts related to the Kullback-Leibler divergence, which is used throughout the field of machine learning. While quantifying the information distinguishing one distribution from another, it also allows us to derive a family of bounds, including the Pinsker inequality and Theorem \ref{thm:PTI}.





















