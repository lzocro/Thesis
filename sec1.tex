
%\lettrine[lines=4,slope=1pt]{\textcolor{red}{A}}{ll} 
 %\lettrine[lines=3,slope=1pt]{A}{n} 
%
%
%
\chapter{Measure and Information Theory}
%
%
%

\lettrine[lines=4,slope=1pt]{\textcolor{red}{}}{} 
%
%
\section{Measure Theory}
%
\lettrine[lines=3,slope=1pt]{M}{easure theory} allows us to reformulate and generalise many statements about probability distributions. To go beyond naive probability, we will need to rebuild our definitions from new classes of sets. We will begin by introducing measures, then we will extend them to a key result, which will allow us to formulate a generalised probability density, unifying probability mass and density functions.

%
\subsection{Measures}
%

\par We consider first an arbitrary topological space $\mathcal{X}$, and we will take interest in $\mathcal{E}\subset\mathcal{P}\mathcal{X}$, a class of sets on $\mathcal{X}$. We will take the normal operations on sets, and in particular we will denote set-theoretic difference as ``$-$'' and define some behaviours $\mathcal{E}$ can have. 

\begin{definition}[Rings and Fields]
A non empty class $\mathcal{R}$ is a ring if for all $E,F\in \mathcal{R}$: $E\cup F, E-F \in \mathcal{R}$. A field is a ring closed under the complement in $\mathcal{X}$. \\
A ring closed under countable union is called a $\sigma$-ring, and naturally it is a $\sigma$-field or $\sigma$-algebra if it is also closed under complements. 
\end{definition}

In probability we will be mostly interested in $\sigma$-algebras, but measure theory is not restricted to them at all. A particularly important example is the Borel $\sigma$-algebra $\mathcal{B}$ on the real line. Consider the collection of all open intervals on $\RR$, or equivalently all left-open, right-closed intervals. The class of Borel sets for the real line is the $\sigma$-ring generated by this collection. This can be shown to also be a $\sigma$-field, which we denote $\mathcal{B}$. $\mathcal{B}$ contains for instance all one-point and countable sets, as well as all intervals in $\mathbb{R}$. Now that we have clearly defined and explored classes of sets, we move to measuring them, without further adue. 

\begin{definition}[Set functions and their properties]
A map $M$ from $\mathcal{E}$ to some set $S$ is a set function if for every $e\in\mathcal{E}$, $M(e)\in S$. In particular we can say that $M$ is:
\begin{list}{$\circ$}{}
\item Non-negative: if $\forall E\in \mathcal{E}\, :\, M(E)\geq 0$.
\item Countably additive: if for $\{E_i\}_i$ disjoint sets in $\mathcal{E}$, we have $M(\cup_{i=1}^\infty E_i)=\sum_{i=1}^\infty M(E_i)$.
\item Finite: if $\forall e\in \mathcal{E}\, :\, \vert M(E)\vert <\infty$.
\item $\sigma$-finite: if $\forall E\in \mathcal{E}$ there is a sequence of sets $\{E_i\}_i$ in $\mathcal{E}$ with $E\in\cup_{i=1}^\infty E_i$ and $\vert M(E_i)\vert <\infty$ for all $i\in\mathbb{N}$.
\item Real-valued: if $\forall E\in \mathcal{E}\, :\, M(E)\in\mathbb{R}$.
\item Simple: if $M$ is Real valued, $\mathcal{E}=\mathcal{X}$, and $M(E)$ takes a finite number of values. 
\item Monotone: if $\forall E\subset F\in \mathcal{E} \Leftrightarrow M(E)\leq M(F)$.
\item Subtractive: if $\forall E\subset F\in \mathcal{E}$ such that $E-F\in\mathcal{E}$ and $\vert M(E)\vert <\infty$, we have $M(F-E)=M(F)-M(E)$.
\end{list}
\end{definition}

\begin{definition}[Measure]
A measure $\mu$ on $\mathcal{E}$, with $\emptyset\in\mathcal{E}$, is a non-negative, countably additive set-function from $\mathcal{E}$ to $\RR$. If $\mu(\mathcal{X})=1$ then $\mu$ is a probability measure.
\end{definition}

In particular, perhaps the most important measure is the Lebesgue measure on $\mathcal{B}$ defined simply as the difference between the bounds of the interval, the intuitive length: $\mu((a,b])=b-a$. The Lebesgue measure further returns the intuitive length, area, and volume when applied to $n$-dimensional euclidian space.
%)
\begin{definition}[Measurablitiy]
Let $(\mathcal{X},\mathcal{S})$ be a measurable space, i.e. $\mathcal{S}$ is $\sigma$-field on $\mathcal{X}$, a set $E$ is measurable simply if it is an element of $\mathcal{S}$.
\end{definition}

\par To begin expanding measurability from sets to functions, we introduce the measure space $(\mathcal{X},\mathcal{S},\mu)$, where $\mathcal{X}$ is a topological space, $\mathcal{S}\in\mathcal{P}\mathcal{X}$ is a $\sigma$-field, and $\mu$ is a measure on $\mathcal{S}$. We will also extend our example of the Borel sets to the extended real line $\mathbb{R}^*=\mathbb{R}\cup\{-\infty,\infty\}$. The extended Borel $\sigma$-field $\mathcal{B}^*$ is defined as $\{ B, B\cup\{\infty\},B\cup\{-\infty\}, B\cup\{-\infty,\infty\} : B\in\mathcal{B} \}$.

\begin{definition}[$\mathcal{S}$-measurability]
Let $f:(\mathcal{X},\mathcal{S})\mapsto(\mathcal{Y},\mathcal{T})$ be a set-function between two measurable spaces, f is $\mathcal{S}$-measurable if: 
\[\forall E\in\mathcal{T}:\, f^{-1}(E)\in\mathcal{S}\, . \]
In particular if $f$ is real valued we can take $(\mathcal{Y},\mathcal{T}):= (\mathbb{R}^*,\mathcal{B}^*)$ and obtain the following condition:
\[ \forall B\in\mathcal{B}^*:\, f^{-1}(B)\in\mathcal{S}\, .\]
\end{definition}



%
\subsection{The Lebesgue-Radon-Nikodym theorem}
%

\par Before we can introduce the aforementioned theorem, we have to discuss integrability with respect to a measure, as well as signed measure. Integration is where measures will become particularly important to us, as so far they have only been defined. %This counterintuitive construction, which does not require a measure to measure a set or function is in fact well grounded. The use of the structure of the $\sigma$-algebra to define measurability offers some guarantees that we can equip the measurable space with a measure. Not that $\sigma$-algebras are closed under countable union, and we can thus ensure we can construct sequences as required for countable-additivity. Furthermore, every $\sigma$-algebra contains the empty set. There remains only to actually define the measure to ensure it is non-negative and  

\par We begin by the case of integrating simple functions, which we can write as a finite sum of Indicator functions, where $E_i$ is the sub-class of $\mathcal{E}$ where $f$ takes value $a_i$. Then we can define the integral of these functions as a simple finite sum:
\[ \int f \de\mu= \sum_{i=1}^na_i\mu(E_i)\, . \]
This is an important first step as it can be shown that all non-negative measurable functions are the limit of an increasing sequence $\{f_n\}_n$ of non-negative simple functions. Allowing us to formulate a sensible definition for the integral of these functions:
\[\int f \de \mu:=\lim_{n\to \infty} \int f_n\de\mu\, . \]
If the integral as such defined is finite, $f$ is integrable. This definition is consistent with the Riemann integral on the real line, but it needs to be extended to functions with values in $(-\infty,0)$. This is done by separating the function into positive and negative parts $f_+$ and $f_-$. See now that both are non-negative and if they are integrable we have that $f$ is integrable and its integral takes the finite value:
\[\int f\de\mu:=\int f_+\de\mu - \int f_-\de\mu\, .\]

\par To proceed to more interesting properties, we will need to define the term ``almost everywhere'' in regards to a statement $s$. In a fixed measure space, $s$ is said to hold almost everywhere if it holds for a set $A\in\mathcal{S}$ such that $\mu(A^c)=0$. In terms of the measure, the collection of points where the property does not hold is negligeable, which gives the otherwise fuzz ``almost everywhere'' a sensible meaning. 

\begin{theorem}%4.4.1 in leadbetter
If $f$ is integrable, it is finite almost everywhere. 
If $f$ is measurable, defined almost everywhere and $E$ is a set with $0$ measure, then $\int_E f \de\mu=0$. 
\end{theorem}  

%signed measures

%integrability wrt \mu

%dominance (absolute continuity), orthogonality


%
%
\section{Information Theory}
%
%
\lettrine[lines=3,slope=0pt]{H}{aving} solidly established the segments of measure theory that are of interest in this thesis, these can be used to formulate a rigorous formulation of key concepts that will be required later. First, we will dive into the first works in Information Theory, explaining the paradigm and the contents of C. Shannon's seminal work, and formally defining entropy in terms of measures and Radon-Nikodym derivatives. Further on, we will extend this work to the well known KL-divergence and explore it's use as a bound with the Pinsker inequality.

%
\subsection{Information and Divergence}
%

\par The fundamental premise of information theory is in signal processing and deals with the analysis of the transmission of messages over \textit{noisy} channels. To by-pass the random scrambling of characters in the channel, the two parties can agree on a common \textit{code}. The source then encodes it's message into an object which is transmitted over the channel to the destination's decoder. In the real world, and signal processing there is a cost to a more complicated code in terms of the speed of transfer of the message but in applications to statistics we are interested in the point of view of the decoder. In a stochastic environment, say we receive $x$ and we are tasked with determining if it comes from distribution $f_1$ of $f_2$. What amount of \textit{information} does $x$ contain about differentiating $f_1$, $f_2$? 

Resuming our experiment, let us formulate using Bayes' theorem the posterior probability of a Hypothesis $i=1,2$. corresponding to $x$ belonging to $f_i$. 
\[P(H_i\vert x) = \frac{P(H_i)f_i(x)}{P(H_1)f_1(x)+P(H_2)f_2(x)}\, . \]
We can rearrange the logarithm of the ratio of posteriors for $i=1,2$ into a formula that holds almost everywhere w.r.t $\mu$.
\[ \ln\frac{f_1(x)}{f_2(x)}=\ln\frac{P(H_1\vert x)}{P(H_2\vert x)}-\ln\frac{P(H_1)}{P(H_2)}\, . \]
Since the right-hand side is a measure of the change in log-odds between $H_1$, $H_2$ before and after $x$ is observed, the left hand side is too, albeit in a less obvious way. We call the left-hand side the information contained in the event $\{X(\omega)=x\}$ for differentiating $H_1$ and $H_2$.  We can then define the mean information using the generalised probability densities $f_1$, $f_2$ for $\mu_1(E)\neq0$:
\begin{align*}
 I(1,2;E)&=\frac{1}{\mu_1(E)}\int_E \ln\frac{f_1(x)}{f_2(x)}\de\lambda_1\\
 &=\frac{1}{\mu_1(E)}\int_E f_1(x)\ln\frac{f_1(x)}{f_2(x)}\de\mu\, .
\end{align*}
The second line follows from the Radon-Nikodym derivative $f_1(x)=\frac{\de\lambda_1}{\de\mu}(x)$. Here we notice that $f$ is not important and instead can be fully described by two metrics, $\mu$ and $\lambda_1$. Thus in the measurable space $(\Omega,\mathcal{F})$ we apply $\mu$ to make it a probability space, and two other measures $\lambda_i$ such that the Radon-Nikodym derivatives of the $\lambda_i$ w.r.t $\mu$ are the densities we are trying to differentiate. Extending our definition from $E$ to $\Omega$:
\[D_{KL}(\lambda_1\Vert\lambda_2):= \int \ln\frac{\lambda_1}{\lambda_2}\de\lambda_1=\int \frac{\lambda_1}{\lambda_2} \ln\frac{\lambda_1}{\lambda_2}\de\mu\, . \]
This quantity is called the \textit{Kullback-Leibler (KL) divergence}. Note that if $f_1$ is not absolutely continuous with respect to $f_2$, the divergence is infinite, but otherwise it is finite. This operator is not symmetric and it is therefore not possible to think of the divergence between two measures, but rather from $\lambda_1$ to $\lambda_2$. The most sensible definition is doubtless the idea which we developed in the simple case as the significance of how helpful information is at separating both measures.  We do not need further details about the properties of the KL-divergence, or about other information theoretic concepts such as entropy. As the reader might have guessed, we are mostly interested in using the probability measures of specific random variables, rather than arbitrary ones. Throughout this thesis an important concept will be bounding a random variable called the \textit{pseudo-regret}, and for one such bound we will require a class of results known as the \textit{Pinsker-type inequalities}. 

%
\subsection{Pinsker-type Inequalities}
%
 
 \par One reason we have developed all these precise tools, is to form an acceptable background for establishing and proving specific results. In particular, we want to look for bounding inequalities. An important bounding equality related to Kullback-Leibler divergence is the Pinsker inequality. For two measures in the measurable space $(\Omega,\mathcal{F})$ as before, we define the total variation distance $\delta(\lambda_1,\lambda_2)=\sup\{ \vert \lambda_1(E) - \lambda_2(E)\vert : E\in\mathcal{F}\}$. Then the aforementioned inequality states that:
 \[ 2\delta(\lambda_1,\lambda_2)^2 \leq D_{KL}(\lambda_1\Vert \lambda_2))\, . \]
 
 
 
 
  This subsection is concerned with only one result, followed by its proof, both of which follow. 


\begin{theorem}[Pinsker-type Inequality]
Let $\lambda_1,\lambda_2$ be probability measures on $(\Omega,\mathcal{F})$. For all $E\in\mathcal{F}$, we have:
\[\lambda_1(E)+\lambda_2(E^c)\geq \frac{1}{2}\exp(-D_{KL}(\lambda_1\Vert \lambda_2))\, . \]
\end{theorem}

\begin{proof}
Consider a probability space $(\Omega,\mathcal{F},\mu)$, let $\lambda_1$ and $\lambda_2$ be two further measures on this space, with Radon-Nikodym derivatives w.r.t $\mu$ $f_1$ and $f_2$ respectively. For $E\in\mathcal{F}$, we begin by reducing the left hand side to an integral of a minimum on $\Omega$. See that:
\begin{align*}
\lambda_1(E)+\lambda_2(E^c) &= \int_Ef_1\de\mu + \int_{E^c}f_2\de\mu\\
&\geq \int_E \min(f_1,f_2)\de\mu + \int_{E^c}\min(f_1,f_2)\de\mu\\
&\geq \int \min(f_1,f_2)\de\mu\,. 
\intertext{Note now that $f_1+f_2=\max(f_1,f_2)+\min(f_1,f_2)$. Using the unit integrability of measures this trivial fact allows us to derive the much more useful statement that $\int \max(f_1,f_2)\de\mu\leq 2-\int\min(f_1,f_2)\de\mu\leq 2$. Now:}
\int \min(f_1,f_2)&\geq \frac{1}{2}\int\min(f_1,f_2)\de\mu\int\max(f_1,f_2)\de\mu \\
&\geq \frac{1}{2}\int f_1\de\mu\int f_2\de\mu \\
&\geq \frac{1}{2}\int (\sqrt{f_1f_2})^2\de\mu\\
&\geq \frac{1}{2}\left(\int\sqrt{f_1f_2}\de\mu\right)^2\\
&\geq \frac{1}{2}\exp\left( 2\ln\int\sqrt{f_1f_2}\de\mu \right)\\
&\geq \frac{1}{2}\exp\left( 2\ln\int f_1\sqrt{\frac{f_2}{f_1}}\de\mu \right)\,.
\intertext{Further as $f_2$ is absolutely continuous w.r.t $f_2$, we know that $f_1>0$ implies $f_1f_2>0$. This will allow us to take the logarithm into the integral:}
\lambda_1(E)+\lambda_2(E^c)&\geq \frac{1}{2}\exp\left( 2\int f_1\ln\sqrt{\frac{f_2}{f_1}}\de\mu \right)\\
&\geq \frac{1}{2}\exp\left( -\int f_1 \ln \frac{f_1}{f_2}\de\mu \right)\,.
\intertext{Replacing $f_1$ and $f_2$ by their definition as Radon-Nikodym derivatives yields the result:}
\lambda_1(E)+\lambda_2(E^c)&\geq \frac{1}{2}\exp\left( -D_{KL}(\lambda_1\Vert \lambda_2)\right)\,.
\end{align*}
\end{proof}
\par This concludes our overview of Measure and information theory, and opens the way for a formal dive into bandit theory. 























